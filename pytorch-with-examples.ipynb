{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 25165598.0102\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "define your neural network. These parameters have to be initially declared.\n",
    "N: input batch size, \n",
    "D_in: Input dimensions(here we have a total of 1000 input data)\n",
    "D_out: Output labels  \n",
    "H : number of hidden units. we have 10 here\n",
    "\"\"\"\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "#Create random input and output data\n",
    "\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "#initialize random weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "#Define Learning rate to update weights\n",
    "learning_rate = 1e-6\n",
    "\n",
    "#We compute the gradients on complete bath rather than individual inputs\n",
    "#run the loop for predefined epochs\n",
    "\n",
    "for run in range(500):\n",
    "    \n",
    "    #Step:1 Forward pass\n",
    "    h = x.dot(w1)\n",
    "    #pass thru non linear activation function. Using RELU\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    \n",
    "    # compute the dot products to output layer. No activation here\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    #Step2: Compute loss\n",
    "    loss = np.square(y - y_pred).sum()\n",
    "    print(run, loss)\n",
    "    \n",
    "    #step3: Gradient Descent: Back prop to compute gradients of w1 and w2\n",
    "    grad_y_pred = 2 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    #update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 layer neural using Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 37735796.56236088\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "\"\"\"\n",
    "N is the batch size\n",
    "D_in is the input dimension\n",
    "D_out is the Output dimension. Number of expected labels\n",
    "h is number of hidden units.\n",
    "\"\"\"\n",
    "N, D_in, D_out, H = 64, 1000, 10, 100\n",
    "\n",
    "#create input and out put matrix\n",
    "\n",
    "x = torch.randn(N, D_in).type(dtype)\n",
    "y = torch.randn(N, D_out).type(dtype)\n",
    "\n",
    "#initialise the weight vector\n",
    "\n",
    "w1 = torch.randn(D_in, H).type(dtype)\n",
    "w2 = torch.randn(H, D_out).type(dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(1):\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min = 0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    print(t, loss)\n",
    "    \n",
    "    #back prop\n",
    "    \n",
    "    grad_y_pred = 2 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_ypred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with pytorch variables - using autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_arr = []\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "N, D_in, D_out, H = 64, 1000, 10, 100\n",
    "\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad = False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad = False)\n",
    "\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad = True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad = True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for i in range(500):\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min = 0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    #print(i, loss.data[0])\n",
    "    loss_arr.append(loss.data[0])\n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "    w1.data = w1.data - learning_rate * w1.grad.data\n",
    "    w2.data = w2.data - learning_rate * w2.grad.data\n",
    "    \n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEDCAYAAAAVyO4LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDQBjhuAGgDFDcAPAmPl/wvoGW5S+/QMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5c53a7d860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.linspace(1, 500, 500)\n",
    "\n",
    "y = loss_arr\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Implementation using Pytorch nn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 661.3726196289062\n",
      "1 613.7214965820312\n",
      "2 573.1510009765625\n",
      "3 537.80810546875\n",
      "4 506.3824157714844\n",
      "5 478.0826110839844\n",
      "6 452.2562255859375\n",
      "7 428.3591613769531\n",
      "8 406.200927734375\n",
      "9 385.59954833984375\n",
      "10 366.1639404296875\n",
      "11 347.920654296875\n",
      "12 330.63787841796875\n",
      "13 314.3023376464844\n",
      "14 298.7052001953125\n",
      "15 283.8702392578125\n",
      "16 269.75140380859375\n",
      "17 256.2711486816406\n",
      "18 243.439208984375\n",
      "19 231.19265747070312\n",
      "20 219.46563720703125\n",
      "21 208.2959747314453\n",
      "22 197.6421661376953\n",
      "23 187.46929931640625\n",
      "24 177.800537109375\n",
      "25 168.5679168701172\n",
      "26 159.73736572265625\n",
      "27 151.30897521972656\n",
      "28 143.28689575195312\n",
      "29 135.64720153808594\n",
      "30 128.36734008789062\n",
      "31 121.45113372802734\n",
      "32 114.88208770751953\n",
      "33 108.62274169921875\n",
      "34 102.66434478759766\n",
      "35 97.02012634277344\n",
      "36 91.67391204833984\n",
      "37 86.61100006103516\n",
      "38 81.81837463378906\n",
      "39 77.28254699707031\n",
      "40 72.98246765136719\n",
      "41 68.90815734863281\n",
      "42 65.04307556152344\n",
      "43 61.37541961669922\n",
      "44 57.908504486083984\n",
      "45 54.636802673339844\n",
      "46 51.54840087890625\n",
      "47 48.63570022583008\n",
      "48 45.890228271484375\n",
      "49 43.29893112182617\n",
      "50 40.859622955322266\n",
      "51 38.554901123046875\n",
      "52 36.37931823730469\n",
      "53 34.32874298095703\n",
      "54 32.39501190185547\n",
      "55 30.571977615356445\n",
      "56 28.85443115234375\n",
      "57 27.240375518798828\n",
      "58 25.71718406677246\n",
      "59 24.281524658203125\n",
      "60 22.929126739501953\n",
      "61 21.657255172729492\n",
      "62 20.461040496826172\n",
      "63 19.334749221801758\n",
      "64 18.272409439086914\n",
      "65 17.27206802368164\n",
      "66 16.328468322753906\n",
      "67 15.439933776855469\n",
      "68 14.601839065551758\n",
      "69 13.810778617858887\n",
      "70 13.065183639526367\n",
      "71 12.36276626586914\n",
      "72 11.700345993041992\n",
      "73 11.076696395874023\n",
      "74 10.488771438598633\n",
      "75 9.934590339660645\n",
      "76 9.412440299987793\n",
      "77 8.91960334777832\n",
      "78 8.454383850097656\n",
      "79 8.015363693237305\n",
      "80 7.6016154289245605\n",
      "81 7.210962295532227\n",
      "82 6.842283248901367\n",
      "83 6.493656158447266\n",
      "84 6.1641435623168945\n",
      "85 5.853145122528076\n",
      "86 5.559240341186523\n",
      "87 5.281511306762695\n",
      "88 5.019204139709473\n",
      "89 4.77081823348999\n",
      "90 4.5360565185546875\n",
      "91 4.3138275146484375\n",
      "92 4.103334426879883\n",
      "93 3.9043023586273193\n",
      "94 3.7157132625579834\n",
      "95 3.5372233390808105\n",
      "96 3.3682861328125\n",
      "97 3.2082486152648926\n",
      "98 3.0564160346984863\n",
      "99 2.912456750869751\n",
      "100 2.7759742736816406\n",
      "101 2.6465487480163574\n",
      "102 2.523683547973633\n",
      "103 2.406942844390869\n",
      "104 2.296260356903076\n",
      "105 2.19105863571167\n",
      "106 2.091092348098755\n",
      "107 1.9962172508239746\n",
      "108 1.9062085151672363\n",
      "109 1.8205854892730713\n",
      "110 1.7389216423034668\n",
      "111 1.6612051725387573\n",
      "112 1.5872808694839478\n",
      "113 1.5170410871505737\n",
      "114 1.4502482414245605\n",
      "115 1.3866395950317383\n",
      "116 1.3261094093322754\n",
      "117 1.2683887481689453\n",
      "118 1.2134015560150146\n",
      "119 1.1609615087509155\n",
      "120 1.111006498336792\n",
      "121 1.0634028911590576\n",
      "122 1.0179790258407593\n",
      "123 0.9746943116188049\n",
      "124 0.9333845973014832\n",
      "125 0.8940057158470154\n",
      "126 0.8564326167106628\n",
      "127 0.8205075860023499\n",
      "128 0.7862597703933716\n",
      "129 0.7535225749015808\n",
      "130 0.7222802639007568\n",
      "131 0.6924502849578857\n",
      "132 0.6639537215232849\n",
      "133 0.6367339491844177\n",
      "134 0.61077481508255\n",
      "135 0.5860268473625183\n",
      "136 0.5623301267623901\n",
      "137 0.539646565914154\n",
      "138 0.5179582238197327\n",
      "139 0.4972161054611206\n",
      "140 0.47736281156539917\n",
      "141 0.458365261554718\n",
      "142 0.440204381942749\n",
      "143 0.4228196442127228\n",
      "144 0.40619081258773804\n",
      "145 0.3902881145477295\n",
      "146 0.37505578994750977\n",
      "147 0.3604698181152344\n",
      "148 0.3464994430541992\n",
      "149 0.3331037759780884\n",
      "150 0.3202691376209259\n",
      "151 0.307962566614151\n",
      "152 0.29616838693618774\n",
      "153 0.2848539650440216\n",
      "154 0.2740049660205841\n",
      "155 0.26360490918159485\n",
      "156 0.2536229193210602\n",
      "157 0.2440490424633026\n",
      "158 0.23486334085464478\n",
      "159 0.2260405570268631\n",
      "160 0.21757790446281433\n",
      "161 0.20945605635643005\n",
      "162 0.20165811479091644\n",
      "163 0.19416368007659912\n",
      "164 0.18697002530097961\n",
      "165 0.1800636202096939\n",
      "166 0.17343167960643768\n",
      "167 0.16707144677639008\n",
      "168 0.16096729040145874\n",
      "169 0.1551004946231842\n",
      "170 0.14946024119853973\n",
      "171 0.14403891563415527\n",
      "172 0.13882896304130554\n",
      "173 0.13382543623447418\n",
      "174 0.1290089637041092\n",
      "175 0.12437612563371658\n",
      "176 0.11991612613201141\n",
      "177 0.11562874168157578\n",
      "178 0.11149638146162033\n",
      "179 0.10751908272504807\n",
      "180 0.10369429737329483\n",
      "181 0.10001203417778015\n",
      "182 0.09647195041179657\n",
      "183 0.09306792169809341\n",
      "184 0.08978672325611115\n",
      "185 0.08662766963243484\n",
      "186 0.08358825743198395\n",
      "187 0.08066022396087646\n",
      "188 0.07783814519643784\n",
      "189 0.07512436807155609\n",
      "190 0.07250797003507614\n",
      "191 0.069985531270504\n",
      "192 0.06755553185939789\n",
      "193 0.06521403789520264\n",
      "194 0.06295850872993469\n",
      "195 0.06078565865755081\n",
      "196 0.058689430356025696\n",
      "197 0.056669801473617554\n",
      "198 0.054723042994737625\n",
      "199 0.05284798517823219\n",
      "200 0.051038604229688644\n",
      "201 0.04929420351982117\n",
      "202 0.04761052131652832\n",
      "203 0.045990344136953354\n",
      "204 0.04442565143108368\n",
      "205 0.04291636869311333\n",
      "206 0.041459742933511734\n",
      "207 0.04005489870905876\n",
      "208 0.03870072588324547\n",
      "209 0.03739317134022713\n",
      "210 0.03613120689988136\n",
      "211 0.034914955496788025\n",
      "212 0.03374041989445686\n",
      "213 0.03260641545057297\n",
      "214 0.03151215240359306\n",
      "215 0.030456997454166412\n",
      "216 0.029437782242894173\n",
      "217 0.028453221544623375\n",
      "218 0.02750345505774021\n",
      "219 0.026587389409542084\n",
      "220 0.025702569633722305\n",
      "221 0.024847999215126038\n",
      "222 0.02402251400053501\n",
      "223 0.023225774988532066\n",
      "224 0.022456752136349678\n",
      "225 0.02171451225876808\n",
      "226 0.02099735289812088\n",
      "227 0.020304303616285324\n",
      "228 0.019634783267974854\n",
      "229 0.018987661227583885\n",
      "230 0.01836310513317585\n",
      "231 0.017759552225470543\n",
      "232 0.017176520079374313\n",
      "233 0.016613323241472244\n",
      "234 0.016068996861577034\n",
      "235 0.015543317422270775\n",
      "236 0.015035332180559635\n",
      "237 0.014544622041285038\n",
      "238 0.014070370234549046\n",
      "239 0.013611888512969017\n",
      "240 0.013168731704354286\n",
      "241 0.01274069957435131\n",
      "242 0.012326776050031185\n",
      "243 0.011926678940653801\n",
      "244 0.011539723724126816\n",
      "245 0.01116575300693512\n",
      "246 0.010804453864693642\n",
      "247 0.010455260053277016\n",
      "248 0.010117616504430771\n",
      "249 0.009791011922061443\n",
      "250 0.009475216269493103\n",
      "251 0.009169874712824821\n",
      "252 0.008874698542058468\n",
      "253 0.008589131757616997\n",
      "254 0.008313064463436604\n",
      "255 0.008046078495681286\n",
      "256 0.007788023911416531\n",
      "257 0.007538334000855684\n",
      "258 0.007296905852854252\n",
      "259 0.007063352037221193\n",
      "260 0.0068375831469893456\n",
      "261 0.0066191344521939754\n",
      "262 0.0064078811556100845\n",
      "263 0.00620337063446641\n",
      "264 0.006005583330988884\n",
      "265 0.005814117379486561\n",
      "266 0.0056290277279913425\n",
      "267 0.005449898540973663\n",
      "268 0.005276658572256565\n",
      "269 0.005108930636197329\n",
      "270 0.004946703091263771\n",
      "271 0.004789672326296568\n",
      "272 0.004637944512069225\n",
      "273 0.004490979947149754\n",
      "274 0.004348916467279196\n",
      "275 0.004211248829960823\n",
      "276 0.004078141879290342\n",
      "277 0.003949209116399288\n",
      "278 0.003824397223070264\n",
      "279 0.0037036945577710867\n",
      "280 0.0035869067069143057\n",
      "281 0.003473737044259906\n",
      "282 0.003364334348589182\n",
      "283 0.003258467186242342\n",
      "284 0.0031559355556964874\n",
      "285 0.003056679619476199\n",
      "286 0.002960678655654192\n",
      "287 0.0028676975052803755\n",
      "288 0.002777655376121402\n",
      "289 0.002690466120839119\n",
      "290 0.0026061218231916428\n",
      "291 0.002524446463212371\n",
      "292 0.002445394406095147\n",
      "293 0.002368887420743704\n",
      "294 0.002294739941135049\n",
      "295 0.0022229603491723537\n",
      "296 0.0021534916013479233\n",
      "297 0.0020862361416220665\n",
      "298 0.002021122956648469\n",
      "299 0.0019581124652177095\n",
      "300 0.001897003618068993\n",
      "301 0.0018378457752987742\n",
      "302 0.0017805988900363445\n",
      "303 0.001725179492495954\n",
      "304 0.0016714917728677392\n",
      "305 0.001619484624825418\n",
      "306 0.0015691353473812342\n",
      "307 0.0015203640796244144\n",
      "308 0.001473117619752884\n",
      "309 0.0014273865381255746\n",
      "310 0.0013831044780090451\n",
      "311 0.0013401876203715801\n",
      "312 0.001298596616834402\n",
      "313 0.0012583857169374824\n",
      "314 0.0012193917063996196\n",
      "315 0.0011816233163699508\n",
      "316 0.001145059592090547\n",
      "317 0.001109606702812016\n",
      "318 0.001075289212167263\n",
      "319 0.0010420416947454214\n",
      "320 0.001009831321425736\n",
      "321 0.0009786119917407632\n",
      "322 0.0009484096080996096\n",
      "323 0.0009191425051540136\n",
      "324 0.0008907808805815876\n",
      "325 0.000863325665704906\n",
      "326 0.00083670619642362\n",
      "327 0.000810919504147023\n",
      "328 0.0007859384641051292\n",
      "329 0.0007617289084009826\n",
      "330 0.0007382840267382562\n",
      "331 0.0007155767525546253\n",
      "332 0.0006935932906344533\n",
      "333 0.0006722650141455233\n",
      "334 0.0006516059511341155\n",
      "335 0.0006315909558907151\n",
      "336 0.0006122062914073467\n",
      "337 0.0005934122600592673\n",
      "338 0.0005752034485340118\n",
      "339 0.0005575621034950018\n",
      "340 0.0005404577823355794\n",
      "341 0.0005239045131020248\n",
      "342 0.0005078467074781656\n",
      "343 0.0004922957741655409\n",
      "344 0.0004772287211380899\n",
      "345 0.00046261760871857405\n",
      "346 0.00044846729724667966\n",
      "347 0.00043475779239088297\n",
      "348 0.0004214647051412612\n",
      "349 0.0004085737746208906\n",
      "350 0.00039608802762813866\n",
      "351 0.000383999285986647\n",
      "352 0.00037226942367851734\n",
      "353 0.0003609130799304694\n",
      "354 0.000349910871591419\n",
      "355 0.0003392402722965926\n",
      "356 0.00032888512942008674\n",
      "357 0.00031886802753433585\n",
      "358 0.0003091531980317086\n",
      "359 0.00029974253266118467\n",
      "360 0.00029061586246825755\n",
      "361 0.00028176791965961456\n",
      "362 0.0002731987042352557\n",
      "363 0.00026489884476177394\n",
      "364 0.0002568391792010516\n",
      "365 0.0002490291662979871\n",
      "366 0.00024146906798705459\n",
      "367 0.00023412478913087398\n",
      "368 0.000227014345000498\n",
      "369 0.0002201295574195683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370 0.0002134506357833743\n",
      "371 0.000206975411856547\n",
      "372 0.0002006955473916605\n",
      "373 0.00019461232295725495\n",
      "374 0.00018871559586841613\n",
      "375 0.00018299039220437407\n",
      "376 0.0001774476986611262\n",
      "377 0.00017207411292474717\n",
      "378 0.0001668605109443888\n",
      "379 0.0001618115056771785\n",
      "380 0.00015691046428401023\n",
      "381 0.00015216450265143067\n",
      "382 0.00014756487507838756\n",
      "383 0.0001431014243280515\n",
      "384 0.00013878107711207122\n",
      "385 0.0001345861965091899\n",
      "386 0.0001305162877542898\n",
      "387 0.00012658091145567596\n",
      "388 0.0001227533648489043\n",
      "389 0.00011904602433787659\n",
      "390 0.0001154577694251202\n",
      "391 0.00011197268759133294\n",
      "392 0.00010859630856430158\n",
      "393 0.00010532479063840583\n",
      "394 0.00010215028305537999\n",
      "395 9.907277853926644e-05\n",
      "396 9.608944674255326e-05\n",
      "397 9.319108357885852e-05\n",
      "398 9.038492135005072e-05\n",
      "399 8.766532846493647e-05\n",
      "400 8.5028528701514e-05\n",
      "401 8.246945799328387e-05\n",
      "402 7.998733781278133e-05\n",
      "403 7.758157880743966e-05\n",
      "404 7.5251649832353e-05\n",
      "405 7.299045682884753e-05\n",
      "406 7.079596252879128e-05\n",
      "407 6.8666398874484e-05\n",
      "408 6.660580402240157e-05\n",
      "409 6.460666918428615e-05\n",
      "410 6.266578566282988e-05\n",
      "411 6.078310252632946e-05\n",
      "412 5.896305083297193e-05\n",
      "413 5.7196459238184616e-05\n",
      "414 5.547934051719494e-05\n",
      "415 5.381786104408093e-05\n",
      "416 5.2202489314367995e-05\n",
      "417 5.0637700041988865e-05\n",
      "418 4.912455551675521e-05\n",
      "419 4.7650974011048675e-05\n",
      "420 4.622569031198509e-05\n",
      "421 4.484121018322185e-05\n",
      "422 4.349839218775742e-05\n",
      "423 4.219737456878647e-05\n",
      "424 4.093247480341233e-05\n",
      "425 3.9711019780952483e-05\n",
      "426 3.852419467875734e-05\n",
      "427 3.737154111149721e-05\n",
      "428 3.6252389691071585e-05\n",
      "429 3.517136428854428e-05\n",
      "430 3.412057412788272e-05\n",
      "431 3.310065949335694e-05\n",
      "432 3.211227522115223e-05\n",
      "433 3.1153365853242576e-05\n",
      "434 3.022366581717506e-05\n",
      "435 2.9321328838705085e-05\n",
      "436 2.8447722797864117e-05\n",
      "437 2.759915696515236e-05\n",
      "438 2.677606971701607e-05\n",
      "439 2.5976822144002654e-05\n",
      "440 2.5203478799085133e-05\n",
      "441 2.445137761242222e-05\n",
      "442 2.372384915361181e-05\n",
      "443 2.301770837220829e-05\n",
      "444 2.2331962099997327e-05\n",
      "445 2.16676780837588e-05\n",
      "446 2.102352664223872e-05\n",
      "447 2.0397828848217614e-05\n",
      "448 1.9791450540651567e-05\n",
      "449 1.920153408718761e-05\n",
      "450 1.863075522123836e-05\n",
      "451 1.8075932530337013e-05\n",
      "452 1.7538710380904377e-05\n",
      "453 1.7018543076119386e-05\n",
      "454 1.6511881767655723e-05\n",
      "455 1.6022355339373462e-05\n",
      "456 1.5546236681984738e-05\n",
      "457 1.5084845472301822e-05\n",
      "458 1.4635991647082847e-05\n",
      "459 1.420215266989544e-05\n",
      "460 1.3781123016087804e-05\n",
      "461 1.3373620276979636e-05\n",
      "462 1.2976030120626092e-05\n",
      "463 1.2591048289323226e-05\n",
      "464 1.2217808034620248e-05\n",
      "465 1.1855006050609518e-05\n",
      "466 1.1505172551551368e-05\n",
      "467 1.1163123417645693e-05\n",
      "468 1.0832259249582421e-05\n",
      "469 1.0511867003515363e-05\n",
      "470 1.020112176775001e-05\n",
      "471 9.898646567307878e-06\n",
      "472 9.606616004020907e-06\n",
      "473 9.322015102952719e-06\n",
      "474 9.04636544873938e-06\n",
      "475 8.778898518357892e-06\n",
      "476 8.519542461726815e-06\n",
      "477 8.267187695309985e-06\n",
      "478 8.023239388421644e-06\n",
      "479 7.786163223499898e-06\n",
      "480 7.55687824494089e-06\n",
      "481 7.3323458309459966e-06\n",
      "482 7.116686902008951e-06\n",
      "483 6.906331236677943e-06\n",
      "484 6.7027622208115645e-06\n",
      "485 6.504456450784346e-06\n",
      "486 6.31307193543762e-06\n",
      "487 6.126114385551773e-06\n",
      "488 5.945598331891233e-06\n",
      "489 5.769854396930896e-06\n",
      "490 5.600381882686634e-06\n",
      "491 5.43503801964107e-06\n",
      "492 5.274989689496579e-06\n",
      "493 5.119244633533526e-06\n",
      "494 4.9691880121827126e-06\n",
      "495 4.822180471819593e-06\n",
      "496 4.680939582613064e-06\n",
      "497 4.543173417914659e-06\n",
      "498 4.409759640111588e-06\n",
      "499 4.279190306988312e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "N, D_in, D_out, H = 64, 1000, 10, 100\n",
    "\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad = False)\n",
    "y = Variable(torch.randn(N, D_out), requires_grad = False)\n",
    "\n",
    "model = torch.nn.Sequential(torch.nn.Linear(D_in, H, bias = True),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(H, D_out, bias=True)\n",
    "        )\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.data[0])\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.data -= learning_rate * param.grad.data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
